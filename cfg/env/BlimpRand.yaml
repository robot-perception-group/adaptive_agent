# ======== Asset info blimp: ========
# Got 15 bodies, 14 joints, and 5 DOFs
# Bodies:
#   0: 'base'
#   1: 'gondola'
#   2: 'blimp_stick'
#   3: 'blimp_wing1'
#   4: 'blimp_wing2'
#   5: 'blimp_support1'
#   6: 'blimp_support2'
#   7: 'blimp_rud_bot'
#   8: 'rud_bot'
#   9: 'blimp_rud_left'
#  10: 'rud_left'
#  11: 'blimp_rud_right'
#  12: 'rud_right'
#  13: 'blimp_rud_top'
#  14: 'rud_top'
# Joints:
#   0: 'base_gondola' (Fixed)
#   1: 'gondola_stick' (Revolute)
#   2: 'stick_wing1' (Fixed)
#   3: 'stick_wing2' (Fixed)
#   4: 'gondola_support1' (Fixed)
#   5: 'gondola_support2' (Fixed)
#   6: 'base_rud_bot' (Fixed)
#   7: 'rud_bot' (Revolute)
#   8: 'base_rud_left' (Fixed)
#   9: 'rud_left' (Revolute)
#  10: 'base_rud_right' (Fixed)
#  11: 'rud_right' (Revolute)
#  12: 'base_rud_top' (Fixed)
#  13: 'rud_top' (Revolute)
# DOFs:
#   0: 'gondola_stick' (Rotation)
#   1: 'rud_bot' (Rotation)
#   2: 'rud_left' (Rotation)
#   3: 'rud_right' (Rotation)
#   4: 'rud_top' (Rotation)


env_name: "BlimpRand"
num_envs: 1024

episode_max_step: 500
total_episodes: 125

mode: "train" # train or test
gazebo: False

eval: True
eval_interval: 5
eval_episodes: 1

save_model: False  # save model after evaluation
log_path: "logs/"  # config, video, model log path
log_interval: 5 # wandb log
log_results: True

reward_scale: 1 
obs_norm: False

blimp:
  spawn_height: 20
  reset_dist: 35

  mass: 8.907
  ema_smooth: [0.2, 0.3] # [thrust, stick]
  # indices of blimp bodies for which aerodynamics is calculated
  drag_body_idxs: [0, 7, 8, 9, 10, 11, 12, 13, 14]
  # approx (x,y,z) components of area for the above bodies (in m^2)
  areas:   [[2.544, 6.3, 6.3], # base
            [0, 0.30, 0], # blimp_rud_bot
            [ 0, 0.15, 0], # rud_bot
            [0, 0.30, 0], # blimp_rud_left
            [ 0, 0.15, 0], # rud_left
            [0, 0.30, 0], # blimp_rud_right
            [ 0, 0.15, 0], # rud_right
            [0, 0.30, 0], # blimp_rud_top
            [ 0, 0.15, 0],] # rud_top
  
  drag_coef: [[0.3, 0.47, 0.47],
              [0,   1.17,    0],
              [0,   1.17,    0],
              [0,   1.17,    0],
              [0,   1.17,    0],
              [0,   1.17,    0],
              [0,   1.17,    0],
              [0,   1.17,    0],
              [0,   1.17,    0],]

aero:
  wind_dirs: [1.0, 1.0, 0.25]
  wind_mag: 1.75
  wind_std: 0.05

goal: 
  type: "rand" # rand, fix

  pos_lim: 20.0
  vel_lim: 4.0
  avel_lim: 0.5

  # fix goal
  style: "hourglass" # square, hourglass, circle

  # rand goal
  kWayPt: 3
  wp_dist: 15
  trigger_dist: 5
  min_z: 10
  max_z: 30

  rand_pos_targets: True
  rand_ang_targets: True
  rand_vel_targets: True
  rand_avel_targets: False

  # targets for play
  target_pos:     [0, 0, 20] # x, y, z
  target_vel:     [0, 0, 0] # vx, vy, vz
  target_velnorm: 2 # [m/s]
  target_ang:     [0, 0, 0] # rpy
  target_angvel:  [0, 0, 0] # wx, wy, wz

task:
  verbose: False

  rand_task: True # fix training task weight
  init_vel: True
  
  domain_rand: False # randomize env latent
  range_a: [0.8, 1.25] # type a var range
  range_b: [0.5, 2.0] # type b var range

  # proximity reward
  proximity_threshold: 7 # [m]

  # specify initial task weight
  taskLabels:  ['planar','Z','trigger','heading',  'proximity','yaw','vnorm',   'vxy','vz',  'bndcost','regRP','regT', 'regS']
  task_wTrain: [1,.5,1,.1, 0,0,0, 0,0, 0,0,0,0]
  task_wEval: [0,0,0,0, 1,0,.1, 0,0, 0,0,0,0]

  # can be "uniform", "permute", "identity", "achievable", "single", "redundant"
  taskSet_train: "achievable10"
  taskSet_eval: "evaluation"
  adaptive_task: False

  # set of task weights
  taskSet: 
    achievable5:   [
      # controller
      [.1,.1,1,0, 0,0,0, 0,0, 0,.01,0,.1],
      [0,0,0,0, 10,0,.01, 0,0, .2,0,.1,0],
      [0,0,0,0, 0,0,0, 1,.5, 0,.01,0,.1],
      [.1,.1,0,-1, 0,0,-1, 0,0, 0,0,0,0], # backward flight for fun

      # navigation
      [.1,.5,.1,1, 0,0,1, 0,0, 0,.01,.01,.1],
      ]

    achievable10:   [
      # controller
      [.1,.1,1,0, 0,0,0, 0,0, 0,.01,0,.1],
      [0,0,0,0, 10,0,.01, 0,0, .2,0,.1,0],
      [0,0,0,0, 0,0,0, 1,.5, 0,.01,0,.1],
      [.1,.1,0,-1, 0,0,-1, 0,0, 0,0,0,0], # backward flight for fun

      # navigation
      [.1,.5,.1,1, 0,0,1, 0,0, 0,.01,.01,.1],
      # hover
      [0,0,0,0, 10,.2,0, 0,0, .5,0,.5,0],
      # path follow
      [0,0,.1,.1, 0,0,0, 1,.5, 0,.01,0,0],

      # others
      [0,0,0,0, 0,1,0, 0,0, 0,0,.1,0], # improve yaw control
      [1,.1,0,.1, 0,0,0, 0,0, 0,0,0,0], # improve position control
      [0,0,0,0, 1,0,0, 0,0, 0,0,0,0], # improve hovering
      ]

    achievable20:   [
      # controller
      [.1,.1,1,0, 0,0,0, 0,0, 0,.01,0,.1],
      [0,0,0,0, 10,0,.01, 0,0, .2,0,.1,0],
      [0,0,0,0, 0,0,0, 1,.5, 0,.01,0,.1],
      [.1,.1,0,-1, 0,0,-1, 0,0, 0,0,0,0], # backward flight for fun

      # navigation
      [.1,.5,.1,1, 0,0,1, 0,0, 0,.01,.01,.1],
      # hover
      [0,0,0,0, 10,.2,0, 0,0, .5,0,.5,0],
      # path follow
      [0,0,.1,.1, 0,0,0, 1,.5, 0,.01,0,0],

      # others
      [0,0,0,0, 0,1,0, 0,0, 0,0,.1,0], # improve yaw control
      [1,.1,0,.1, 0,0,0, 0,0, 0,0,0,0], # improve position control
      [0,0,0,0, 1,0,0, 0,0, 0,0,0,0], # improve hovering

      # basics
      [1,0,0,0, 0,0,0, 0,0, 0,0,0,.2], # improve position and overshoot control
      [1,0,0,0, 0,0,0, 0,0, 0,0,.3,0], # improve position control with thurst cost
      [0,0,0,0, 1,0,0, 0,0, 1,0,0,0], # improve hover control with bnd cost
      [0,0,0,0, 0,0,1, 0,0, 0,0,0,0], # improve velocity norm control
      [0,0,0,1, 0,0,1, 0,0, 0,0,0,0], # improve velocity norm and heading control
      [0,0,0,1, 0,0,0, 0,0, 0,0,0,0], # improve heading control
      [0,0,0,0, 0,0,0, 1,0, 0,0,0,0], # improve velocity xy control
      [0,0,0,0, 0,0,0, 1,0, 0,0,0,.5], # improve velocity and overshoot control
      [0,0,0,0, 0,0,0, 0,0, 0,0,1,0], # improve thrust cost
      [0,0,0,0, 0,0,0, 0,0, 0,0,-1,0], # encourage thrust
      ]


    evaluation: [
      [1,0,0,0, 0,0,0, 0,0, 0,0,0,0],
      [0,0,1,0, 0,0,0, 0,0, 0,0,0,0],
      [0,0,0,1, 0,0,1, 0,0, 0,0,0,0],
      [0,0,0,0, 1,0,0, 0,0, 0,0,0,0],
      [0,0,0,0, 1,1,0, 0,0, 0,0,0,0],
      [0,0,0,0, 0,0,0, 1,0, 0,0,0,0],
      [0,0,0,0, 0,0,0, 1,.5, 0,0,0,0],
    ]

    single: [
      [0,0,1,0, 0,0,0, 0,0, 0,0,0,0],
    ]

  
  # for PPO/single task training and logging
  single_task: "forward"
  log_rewards_dict: {"forward" : ["return"],}

feature: 
  verbose: False

  scale_pos: 20
  scale_prox: 10
  scale_ang: 50

  
sim:
  dt: 0.05
  substeps: 2
  gravity: [0.0, 0.0, -9.8]
  sim_device: "cuda:0"
  headless: True # set False to visualize 
  compute_device_id: 0
  graphics_device_id: 0